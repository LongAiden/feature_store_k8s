# Use a specific, version-pinned Bitnami Spark image as the base.
# This ensures repeatable builds.
FROM bitnami/spark:3.4.1-debian-11-r88

# --- Metadata ---
LABEL maintainer="Your Name"
LABEL description="Custom Spark image with pre-installed Python libraries and JARs for Kafka, Delta Lake, and S3."

# --- Environment Variables for Versions ---
# Easier to update versions here in one place.
ENV DELTA_VERSION="2.4.0"
ENV KAFKA_VERSION="3.4.1"
ENV HADOOP_AWS_VERSION="3.3.4"
ENV AWS_JAVA_SDK_VERSION="1.12.511"

# --- Installation ---
# Switch to root to install dependencies.
USER root

# Combine all installations into a single RUN command to reduce image layers.
RUN \
    # Step 1: Install system dependencies required for downloads and potential Python package builds.
    apt-get update && \
    apt-get install -y --no-install-recommends wget curl unzip build-essential && \
    \
    # Step 2: Download and install required Java JARs directly into Spark's jars directory.
    # This makes them automatically available to all Spark jobs.
    # -- Delta Lake Connector --
    wget -P /opt/bitnami/spark/jars/ https://repo1.maven.org/maven2/io/delta/delta-core_2.12/${DELTA_VERSION}/delta-core_2.12-${DELTA_VERSION}.jar && \
    wget -P /opt/bitnami/spark/jars/ https://repo1.maven.org/maven2/io/delta/delta-storage/${DELTA_VERSION}/delta-storage-${DELTA_VERSION}.jar && \
    \
    # -- Spark-Kafka Connector --
    wget -P /opt/bitnami/spark/jars/ https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/${KAFKA_VERSION}/spark-sql-kafka-0-10_2.12-${KAFKA_VERSION}.jar && \
    wget -P /opt/bitnami/spark/jars/ https://repo1.maven.org/maven2/org/apache/spark/spark-token-provider-kafka-0-10_2.12/${KAFKA_VERSION}/spark-token-provider-kafka-0-10_2.12-${KAFKA_VERSION}.jar && \
    wget -P /opt/bitnami/spark/jars/ https://repo1.maven.org/maven2/org/apache/commons/commons-pool2/2.11.1/commons-pool2-2.11.1.jar && \
    \
    # -- S3/MinIO Connector (Hadoop-AWS) --
    wget -P /opt/bitnami/spark/jars/ https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/${HADOOP_AWS_VERSION}/hadoop-aws-${HADOOP_AWS_VERSION}.jar && \
    wget -P /opt/bitnami/spark/jars/ https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/${AWS_JAVA_SDK_VERSION}/aws-java-sdk-bundle-${AWS_JAVA_SDK_VERSION}.jar && \
    \
    # Step 3: Install Python dependencies using pip.
    # We create a temporary requirements.txt to keep the command clean.
    echo " \
        python-dotenv==1.0.0 \
        numpy \
        pandas \
        nltk==3.8.1 \
        kafka-python==2.0.2 \
        minio==7.1.16 \
        delta-spark==2.4.0 \
        confluent-kafka==2.3.0 \
        sqlalchemy \
        psycopg2-binary \
        feast==0.36.0 \
        feast-spark \
    " > /tmp/requirements.txt && \
    pip install --no-cache-dir --upgrade pip && \
    pip install --no-cache-dir -r /tmp/requirements.txt && \
    \
    # Step 4: Clean up to keep the image size down.
    apt-get purge -y --auto-remove wget curl unzip build-essential && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/* /tmp/*

# Switch back to the non-root user that the Bitnami image uses.
USER 1001