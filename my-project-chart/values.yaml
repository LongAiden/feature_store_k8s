global:
  imageRegistry: "docker.io" # Or your private registry (e.g., gcr.io, your-harbor.io)

# -------------------------------------------------------------------
# Airflow Configuration (Celery Executor)
# -------------------------------------------------------------------
airflow:
  image:
    # IMPORTANT: Point to your custom-built Airflow image repository
    repository: adrienlong/airflow
    tag: "latest"
    pullPolicy: IfNotPresent

  fernetKey:
    existingSecret: "airflow-secrets"
    key: "fernet-key"
  webserverSecretKey:
    existingSecret: "airflow-secrets"
    key: "webserver-secret-key"

  uid: 50000

  env:
    - name: AIRFLOW__CORE__EXECUTOR
      value: "CeleryExecutor"
    - name: AIRFLOW_DB_HOST
      value: "{{ .Release.Name }}-airflowPostgresql"
    - name: AIRFLOW_DB_PORT
      value: "5432"
    - name: AIRFLOW_DB_USER
      value: "airflow"
    - name: AIRFLOW_DB_NAME
      value: "airflow"
    - name: AIRFLOW_DB_PASSWORD
      valueFrom:
        secretKeyRef:
          name: "airflow-postgresql-secret"
          key: postgresql-password

  # --- Auto-generate Kubernetes connection for KubernetesPodOperator ---
  connections:
    - conn_id: kubernetes_default
      conn_type: kubernetes
      extra: '{"in_cluster":true,"namespace":"{{ .Release.Namespace }}"}'

  extraVolumes:
    - name: scripts-volume
      persistentVolumeClaim:
        # Reference the PVC created by templates/airflow-pvc.yaml
        claimName: "{{ include "k8s-dataflow-project.fullname" . }}-airflow-scripts-pvc"
  extraVolumeMounts:
    - name: scripts-volume
      mountPath: /opt/airflow/scripts # Match the path used in docker-compose for consistency
      readOnly: false

  persistence:
    scripts:
      enabled: true
      accessMode: ReadWriteMany # Or ReadWriteOnce, depending on your storage
      size: 5Gi

  webserver:
    resources:
      requests:
        cpu: 500m # Was 1000m, reduced for overall fit
        memory: 768Mi # Was 1Gi, slight reduction
      limits:
        cpu: 1000m
        memory: 1.5Gi
  scheduler:
    resources:
      requests:
        cpu: 250m # Keep as is, scheduler can be busy
        memory: 512Mi # Increased for stability, was 256Mi
      limits:
        cpu: 500m
        memory: 1Gi
  worker:
    resources:
      requests:
        cpu: 750m # Was 500m, increased if you expect heavy task execution
        memory: 1.5Gi # Was 1Gi, increased as workers can be memory-intensive
      limits:
        cpu: 1500m
        memory: 3Gi
  triggerer:
    resources:
      requests:
        cpu: 150m # Was 250m, reduced
        memory: 256Mi # Keep as is
      limits:
        cpu: 300m
        memory: 512Mi
  init:
    resources:
      requests:
        cpu: 50m # Was 100m, reduced
        memory: 64Mi # Was 128Mi, reduced
      limits:
        cpu: 100m
        memory: 128Mi
  cli:
    resources:
      requests:
        cpu: 50m # Was 100m, reduced
        memory: 64Mi # Was 128Mi, reduced
      limits:
        cpu: 100m
        memory: 128Mi

# -------------------------------------------------------------------
# Spark Standalone Cluster Configuration
# -------------------------------------------------------------------
spark:
  enabled: true
  image:
    repository: adrienlong/spark
    tag: "latest"
    pullPolicy: IfNotPresent
  
  extraVolumes:
    - name: scripts-volume
      persistentVolumeClaim:
        # Reference the PVC created by templates/spark-pvc.yaml for scripts
        claimName: "{{ include "k8s-dataflow-project.fullname" . }}-scripts-pvc"
  extraVolumeMounts:
    - name: scripts-volume
      mountPath: /opt/spark/scripts # Consistent path for Spark scripts
      readOnly: false

  persistence:
    scripts:
      enabled: true
      accessMode: ReadWriteMany # Or ReadWriteOnce, depending on your storage
      size: 5Gi
      # storageClass: "your-storage-class"
    warehouse: # For Spark SQL/Hive warehouse
      enabled: true
      accessMode: ReadWriteMany
      size: 10Gi

  master:
    replicas: 1
    resources:
      requests:
        cpu: 250m # Was 500m, reduced
        memory: 512Mi # Was 1Gi, reduced
      limits:
        cpu: 500m
        memory: 1Gi
  worker:
    replicas: 1 # At least 2 workers are recommended, but for 6 cores, 1 is safer
    resources:
      requests:
        cpu: 1000m # Keep high, Spark workers are core processing units
        memory: 2Gi # Keep high, Spark workers are memory-intensive
      limits:
        cpu: 2000m # Allow burst to 2 cores
        memory: 4Gi

  properties: |
    spark.jars.packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.1
    spark.hadoop.fs.s3a.endpoint http://{{ .Release.Name }}-minio:9000
    spark.hadoop.fs.s3a.path.style.access true
    spark.sql.catalogImplementation hive
    spark.sql.hive.metastore.uris thrift://{{ .Release.Name }}-hive-metastore:9083
  
  s3Credentials:
    accessKey:
      valueFrom:
        secretKeyRef:
          name: "minio-credentials" # Reference the MinIO credentials secret
          key: "root-user" # Key for the MinIO root username
    secretKey:
      valueFrom:
        secretKeyRef:
          name: "minio-credentials" # Reference the MinIO credentials secret
          key: "root-password" # Key for the MinIO root password
# -------------------------------------------------------------------
# --- Data Streaming & CDC ---
# -------------------------------------------------------------------
kafkaConnect:
  enabled: true
  image:
    registry: "{{ .Values.global.imageRegistry }}"
    repository: "adrienlong/debezium"
    tag: "1.0.0"
    pullPolicy: IfNotPresent

  bootstrapServers: "PLAINTEXT://{{ .Release.Name }}-kafka:9092"

  resources:
    requests:
      cpu: 250m # Was 500m, reduced
      memory: 512Mi # Keep as is, Debezium can use this
    limits:
      cpu: 500m
      memory: 1Gi
  
  connectors:
    postgres-cdc: # The name of the connector
      connector.class: "io.debezium.connector.postgresql.PostgresConnector"
      tasks.max: "1"
      database.hostname: "{{ .Release.Name }}-airflowPostgresql" # Connect to the Airflow DB
      database.port: "5432"
      database.user: "airflow"
      
      database.password:
        valueFrom:
          secretKeyRef:
            name: "airflow-postgresql-secret"
            key: postgresql-password
      database.dbname: "airflow"
      database.server.name: "dbserver1"
      plugin.name: "pgoutput"
      slot.name: "debezium_slot"
      publication.name: "debezium_publication"
      database.history.kafka.bootstrap.servers: "PLAINTEXT://{{ .Release.Name }}-kafka:9092"
      database.history.kafka.topic: "dbhistory.postgres"
      table.include.list: "public.transaction_data"
      topic.prefix: "postgres"
      schema.history.internal.kafka.bootstrap.servers: "PLAINTEXT://{{ .Release.Name }}-kafka:9092"
      schema.history.internal.kafka.topic: "schema-changes.postgres"
      decimal.handling.mode: "string"
      heartbeat.interval.ms: "5000"
      tombstones.on.delete: "false"
      topic.creation.default.retention.ms: "86400000"
      topic.creation.default.replication.factor: "1"
      topic.creation.default.partitions: "1"
      max.batch.size: "1024"
      max.queue.size: "4096"

# -------------------------------------------------------------------
# CI/CD - Jenkins (Example from your file)
# -------------------------------------------------------------------
jenkins:
  controller:
    persistence:
      size: "10Gi"
    service:
      type: NodePort
    # Use existingSecret for production
    admin:
      user: "admin"
      password:
        valueFrom:
          secretKeyRef:
            name: "jenkins-admin-secret" # Name of your Kubernetes secret
            key: "admin-password" # Key within the secret for the admin password
    installPlugins:
      - kubernetes:4156.v83c1383dc5a_d
      - workflow-aggregator:596.v802b_b_4fd69b_d
      - git:6.0.0
      - configuration-as-code:1781.v04542759a_1b_3
  resources: # Jenkins controller resources
    requests:
      cpu: 500m
      memory: 1Gi
    limits:
      cpu: 1000m
      memory: 2Gi

# -------------------------------------------------------------------
# Metrics & Monitoring
# -------------------------------------------------------------------
grafana:
  adminPassword: # Moved to secret reference
    valueFrom:
      secretKeyRef:
        name: "grafana-admin-secret"
        key: "admin-password"
  persistence:
    enabled: true
    size: 5Gi

  resources: # Grafana resources
    requests:
      cpu: 50m # Was 100m, reduced. Grafana is usually light until you open many dashboards.
      memory: 128Mi # Was 256Mi, reduced.
    limits:
      cpu: 150m
      memory: 256Mi

  datasources:
    datasources.yaml:
      apiVersion: 1
      datasources:
      - name: InfluxDB
        type: influxdb
        url: http://{{ .Release.Name }}-influxdb:8086
        access: proxy
        isDefault: true
        jsonData:
          version: "Flux"
          organization: "my-org"
          defaultBucket: "my-bucket"
        # Reference the secret created by the InfluxDB chart
        secureJsonData:
          token:
            valueFrom:
              secretKeyRef:
                name: "{{ .Release.Name }}-influxdb-auth"
                key: token # Key name for the token within the secret

influxdb:
  auth:
    admin:
      user: "influx-admin"
      # The chart will create a secret with a random password if not set
  config:
    org: "my-org"
    bucket: "my-bucket"
    retention_period: "720h" # 30 days retention
  resources: 
    requests:
      cpu: 150m 
      memory: 384Mi
    limits:
      cpu: 300m
      memory: 768Mi


# -------------------------------------------------------------------
# Minio (S3-compatible Object Storage)
# -------------------------------------------------------------------
minio:
  auth:
    rootUser:
      valueFrom:
        secretKeyRef:
          name: "minio-credentials" # Reference the secret
          key: "root-user" # Key for the MinIO root username
    rootPassword:
      valueFrom:
        secretKeyRef:
          name: "minio-credentials" # Reference the secret
          key: "root-password" # Key for the MinIO root password

  persistence:
    size: 20Gi

  resources:
    requests:
      cpu: 100m
      memory: 128Mi
    limits:
      cpu: 250m
      memory: 256Mi

# -------------------------------------------------------------------
# Kafka Cluster (Using Bitnami chart conventions)
# -------------------------------------------------------------------
kafka:
  replicaCount: 1 # For production, use 3
  zookeeper:
    replicaCount: 1 # For production, use 3
    resources:
      requests:
        cpu: 100m
        memory: 128Mi
      limits:
        cpu: 250m
        memory: 256Mi
  resources:
    requests:
      cpu: 500m
      memory: 512Mi
    limits:
      cpu: 1000m
      memory: 1Gi

  listeners:
    client:
      protocol: PLAINTEXT
  advertisedListeners:
    - "PLAINTEXT://{{ .Release.Name }}-kafka:9092"

# -------------------------------------------------------------------
# KafKa UI
# -------------------------------------------------------------------
kafkaUI:
  enabled: true
  image:
    # Use the image specified in docker-airflow.yaml
    registry: docker.io
    repository: provectuslabs/kafka-ui
    tag: latest
    pullPolicy: IfNotPresent
  resources:
    requests:
      cpu: 100m
      memory: 128Mi
    limits:
      cpu: 250m
      memory: 256Mi
  # Map docker-compose environment variables to Bitnami Kafka UI chart config
  config:
    clusters:
      - name: local
        bootstrapServers: "PLAINTEXT://{{ .Release.Name }}-kafka:9092"

  auth:
    enabled: false

# -------------------------------------------------------------------
# PostgreSQL Database for Airflow (Sub-chart of Airflow)
# -------------------------------------------------------------------
airflowPostgresql:
  enabled: true
  auth:
    database: "airflow"
    username: "airflow"
    password:
      valueFrom:
        secretKeyRef:
          name: "airflow-postgresql-secret"
          key: "postgresql-password"
  primary:
    persistence:
      size: 8Gi
    resources: # Mapped from postgres in docker-compose
      requests:
        cpu: 250m
        memory: 256Mi
      limits:
        cpu: 500m
        memory: 512Mi


# -------------------------------------------------------------------
# Hive Metastore and its Database
# -------------------------------------------------------------------
hiveMetastore:
  database:
    create: true
    type: "postgres"
    host: "{{ .Release.Name }}-hive-postgresql"
    port: 5432
    user: "hive"
    password: # Moved to secret reference
      valueFrom:
        secretKeyRef:
          name: "hive-metastore-db-secret" # Matches the secret for hivePostgresql
          key: "db-password"
    dbName: "metastore"
  s3:
    endpoint: "http://{{ .Release.Name }}-minio:9000"
    accessKey:
      valueFrom:
        secretKeyRef:
          name: "hive-metastore-s3-secret" # Name of your Kubernetes secret
          key: "access-key" # Key within the secret for the access key
    secretKey:
      valueFrom:
        secretKeyRef:
          name: "hive-metastore-s3-secret" # Name of your Kubernetes secret
          key: "secret-key" # Key within the secret for the secret key
  resources:
    requests:
      cpu: 150m 
      memory: 256Mi 
    limits:
      cpu: 300m
      memory: 512Mi
      
hivePostgresql:
  auth:
    database: "metastore"
    username: "hive"
    password:
      valueFrom:
        secretKeyRef:
          name: "hive-metastore-db-secret" 
          key: "db-password"
  primary:
    persistence:
      size: 5Gi

ingress:
  enabled: true
  className: "nginx"
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /

  hosts:
    - host: grafana.fsk8s.com
      service:
        name: "{{ .Release.Name }}-grafana" # Corrected service name
        port: 80
      tls:
        enabled: true
        secretName: grafana-tls-secret

    - host: minio.fsk8s.com
      service:
        name: "{{ .Release.Name }}-minio" 
        port: 9001 # Port for the MinIO UI, not the API port
      tls:
        enabled: true
        secretName: minio-tls-secret

    - host: kafka.fsk8s.com
      service:
        name: "{{ .Release.Name }}-kafka-ui" 
        port: 8080
      tls:
        enabled: true
        secretName: kafka-tls-secret

    - host: influxdb.fsk8s.com
      service:
        name: "{{ .Release.Name }}-influxdb"
        port: 8086
      tls:
        enabled: true
        secretName: influxdb-tls-secret
